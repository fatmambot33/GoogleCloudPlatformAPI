"""
This type stub file was generated by pyright.
"""

import datetime
import decimal
import pandas as pd
from dataclasses import dataclass
from typing import Any, Dict, List, Literal, Optional, Tuple, Union
from google.cloud import bigquery
from google.cloud.bigquery import QueryJobConfig

"""Utilities for interacting with Google BigQuery."""
DATA_TYPE_MAPPING = ...

class BigQuery:
    """
    High level helper for common BigQuery operations.

    Attributes
    ----------
    SCOPES : list[str]
        The scopes required for the BigQuery API.

    Methods
    -------
    execute_query(query, job_config=None)
        Execute a SQL query and return the resulting rows.
    execute_stored_procedure(sp_name, sp_params)
        Execute a stored procedure and return its result set.
    table_exists(table_id)
        Check whether a BigQuery table exists.
    create_schema_from_table(folder, dataset=None)
        Create a schema definition file based on an existing table.
    create_external_table(dataset_name, table_name, table_schema, source_uris, partition_field="date", time_partioning=False)
        Create an external table based on objects stored in Cloud Storage.
    create_table_from_schema(folder, dataset=None, data_path=None)
        Create a BigQuery table using a stored schema file.
    load_from_query(query, table_id, write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE)
        Execute a query and write results to a table.
    delete_partition(table_id, partition_date, partition_name="date")
        Delete a partition from a table if it exists.
    load_from_cloud(bucket_name, data_set, table, local_folder, remote_folder, partition_date, partition_name="date", file_mask="*.gz", override=False)
        Load data from Cloud Storage into a BigQuery table.
    load_from_local(bucket_name, data_set, table, local_folder, prefix, partition_date, partition_name="date", file_mask="*.csv.gz", override=False)
        Upload local files to Cloud Storage and load them into BigQuery.
    load_from_uri(table_id, bucket_name, data_path, partition_date)
        Load data from a specific Cloud Storage URI.
    build_job_config(table_name, bucket_name, data_path, partition_date)
        Build a BigQuery load job configuration.
    sync_from_cloud(bucket_name, data_set, table, local_folder, remote_folder, partition_date, partition_name="date", override=False)
        Synchronise data from Cloud Storage into BigQuery.
    sync_from_local(bucket_name, data_set, table, local_folder, prefix, partition_date, partition_name="date", file_mask="*.csv.gz", override=False)
        Upload local files then load them into BigQuery.
    bigquery_to_dataframe(query_string)
        Run a query and return the results as a DataFrame.
    dataframe_to_bigquery(dataframe, table_id, write_disposition="WRITE_TRUNCATE")
        Load a DataFrame into a BigQuery table.
    """

    _client: bigquery.Client
    SCOPES = ...
    def __init__(
        self, credentials: Optional[str] = ..., project_id: Optional[str] = ...
    ) -> None:
        """
        Initialise the BigQuery client.

        Parameters
        ----------
        credentials : str, optional
            Path to a service account JSON file. Defaults to ``None`` which
            relies on environment configuration.
        project_id : str, optional
            Google Cloud project identifier. Defaults to ``None``.
        """
        ...

    def __enter__(self) -> BigQuery:
        """
        Return context manager instance.

        Returns
        -------
        BigQuery
            The BigQuery client instance.
        """
        ...

    def __exit__(self, exc_type: Any, exc_val: Any, exc_tb: Any) -> None:
        """
        Close the BigQuery client when leaving the context.

        Parameters
        ----------
        exc_type : Any
            The exception type.
        exc_val : Any
            The exception value.
        exc_tb : Any
            The traceback.
        """
        ...

    def execute_query(
        self, query: str, job_config: Optional[bigquery.QueryJobConfig] = ...
    ) -> List[bigquery.Row]:
        """
        Execute a SQL query and return the resulting rows.

        Parameters
        ----------
        query : str
            SQL query to execute.
        job_config : google.cloud.bigquery.QueryJobConfig, optional
            Optional job configuration used when submitting the query.

        Returns
        -------
        list[google.cloud.bigquery.Row]
            Query results as a list of rows.
        """
        ...

    @dataclass
    class StoredProcedureParameter:
        """
        Parameter description for stored procedure execution.

        Attributes
        ----------
        name : str
            The name of the parameter.
        value : str or int or float or decimal.Decimal or bool or datetime.datetime or datetime.date
            The value of the parameter.
        type : str
            The BigQuery type of the parameter.
        """

        name: str
        value: Union[
            str, int, float, decimal.Decimal, bool, datetime.datetime, datetime.date
        ]
        type: str
        ...

    def execute_stored_procedure(
        self, sp_name: str, sp_params: List[StoredProcedureParameter]
    ) -> pd.DataFrame:
        """
        Execute a stored procedure and return its result set.

        Parameters
        ----------
        sp_name : str
            Fully qualified stored procedure name.
        sp_params : list[BigQuery.StoredProcedureParameter]
            Parameters passed to the stored procedure.

        Returns
        -------
        pandas.DataFrame
            Data frame containing the stored procedure output.
        """
        ...

    def table_exists(self, table_id: str) -> bool:
        """
        Check whether a BigQuery table exists.

        Parameters
        ----------
        table_id : str
            Fully qualified table identifier.

        Returns
        -------
        bool
            ``True`` if the table exists, ``False`` otherwise.
        """
        ...

    def create_schema_from_table(
        self, folder: str, dataset: Optional[str] = ...
    ) -> Optional[Dict[str, Any]]:
        """
        Create a schema definition file based on an existing table.

        Parameters
        ----------
        folder : str
            Table name whose schema will be exported.
        dataset : str, optional
            Dataset containing the table. Defaults to environment variable
            ``DEFAULT_BQ_DATASET``.

        Returns
        -------
        dict or None
            Schema dictionary if the table exists, otherwise ``None``.
        """
        ...

    def create_external_table(
        self,
        dataset_name: str,
        table_name: str,
        table_schema: Dict[str, Any],
        source_uris: List[str],
        partition_field: str = ...,
        time_partioning: bool = ...,
    ) -> bool:
        """
        Create an external table based on objects stored in Cloud Storage.

        Parameters
        ----------
        dataset_name : str
            Target dataset name.
        table_name : str
            Name of the external table to create.
        table_schema : dict
            Schema definition created by :meth:`create_schema_from_table`.
        source_uris : list[str]
            Cloud Storage URIs that hold the external data.
        partition_field : str, optional
            Field used for time partitioning. Defaults to ``"date"``.
        time_partioning : bool, optional
            Whether to enable time partitioning. Defaults to ``False``.

        Returns
        -------
        bool
            ``True`` if the table was created.
        """
        ...

    def create_table_from_schema(
        self, folder: str, dataset: Optional[str] = ..., data_path: Optional[str] = ...
    ) -> bool:
        """
        Create a BigQuery table using a stored schema file.

        Parameters
        ----------
        folder : str
            Folder containing the ``schema.json`` file.
        dataset : str, optional
            Dataset where the table should be created. Defaults to environment
            variable ``DEFAULT_BQ_DATASET``.
        data_path : str, optional
            Base path to schema files. Defaults to environment variable
            ``DATA_PATH``.

        Returns
        -------
        bool
            ``True`` if the table was created.
        """
        ...

    def load_from_query(
        self,
        query: str,
        table_id: str,
        write_disposition: bigquery.WriteDisposition = ...,
    ) -> None:
        """
        Execute a query and write results to a table.

        Parameters
        ----------
        query : str
            The query to execute.
        table_id : str
            The destination table ID.
        write_disposition : bigquery.WriteDisposition, optional
            Specifies the action that occurs if the destination table already
            exists. Defaults to ``WRITE_TRUNCATE``.
        """
        ...

    def delete_partition(
        self, table_id: str, partition_date: datetime.date, partition_name: str = ...
    ) -> bool:
        """
        Delete a partition from a table if it exists.

        Parameters
        ----------
        table_id : str
            The ID of the table.
        partition_date : datetime.date
            The date of the partition to delete.
        partition_name : str, optional
            The name of the partition column. Defaults to 'date'.

        Returns
        -------
        bool
            ``True`` if the partition was deleted, ``False`` otherwise.
        """
        ...

    def load_from_cloud(
        self,
        bucket_name: str,
        data_set: str,
        table: str,
        local_folder: str,
        remote_folder: str,
        partition_date: datetime.date,
        partition_name: str = ...,
        file_mask: str = ...,
        override: bool = ...,
    ) -> bool:
        """
        Load data from Cloud Storage into a BigQuery table.

        Parameters
        ----------
        bucket_name : str
            The name of the Cloud Storage bucket.
        data_set : str
            The BigQuery dataset ID.
        table : str
            The BigQuery table ID.
        local_folder : str
            The local folder path.
        remote_folder : str
            The remote folder path in the bucket.
        partition_date : datetime.date
            The partition date.
        partition_name : str, optional
            The name of the partition column. Defaults to 'date'.
        file_mask : str, optional
            The file mask to load. Defaults to '*.gz'.
        override : bool, optional
            Whether to override the destination partition. Defaults to ``False``.

        Returns
        -------
        bool
            ``True`` if the load was successful.
        """
        ...

    def load_from_local(
        self,
        bucket_name: str,
        data_set: str,
        table: str,
        local_folder: str,
        prefix: str,
        partition_date: datetime.date,
        partition_name: str = ...,
        file_mask: str = ...,
        override: bool = ...,
    ) -> bool:
        """
        Upload local files to Cloud Storage and load them into BigQuery.

        Parameters
        ----------
        bucket_name : str
            The name of the Cloud Storage bucket.
        data_set : str
            The BigQuery dataset ID.
        table : str
            The BigQuery table ID.
        local_folder : str
            The local folder containing the data.
        prefix : str
            The prefix to use for the remote folder.
        partition_date : datetime.date
            The partition date.
        partition_name : str, optional
            The name of the partition column. Defaults to 'date'.
        file_mask : str, optional
            The file mask of the files to upload. Defaults to '*.csv.gz'.
        override : bool, optional
            Whether to override existing files in the bucket. Defaults to ``False``.

        Returns
        -------
        bool
            ``True`` if the load was successful.
        """
        ...

    def load_from_uri(
        self,
        table_id: str,
        bucket_name: str,
        data_path: str,
        partition_date: datetime.date,
    ) -> bool:
        """
        Load data from a specific Cloud Storage URI.

        Parameters
        ----------
        table_id : str
            The destination table ID.
        bucket_name : str
            The Cloud Storage bucket name.
        data_path : str
            The path to the data within the bucket.
        partition_date : datetime.date
            The partition date to load.

        Returns
        -------
        bool
            ``True`` if the load was successful.
        """
        ...

    @staticmethod
    def build_job_config(
        table_name: str, bucket_name: str, data_path: str, partition_date: datetime.date
    ) -> Tuple[bigquery.LoadJobConfig, str]:
        """
        Build a BigQuery load job configuration.

        Parameters
        ----------
        table_name : str
            The name of the table.
        bucket_name : str
            The name of the bucket.
        data_path : str
            The path to the data.
        partition_date : datetime.date
            The partition date.

        Returns
        -------
        tuple[bigquery.LoadJobConfig, str]
            A tuple containing the job configuration and the source URI.
        """
        ...

    @staticmethod
    def sync_from_cloud(
        bucket_name: str,
        data_set: str,
        table: str,
        local_folder: str,
        remote_folder: str,
        partition_date: datetime.date,
        partition_name: str = ...,
        override: bool = ...,
    ) -> None:
        """
        Synchronise data from Cloud Storage into BigQuery.

        Parameters
        ----------
        bucket_name : str
            The name of the Cloud Storage bucket.
        data_set : str
            The BigQuery dataset ID.
        table : str
            The BigQuery table ID.
        local_folder : str
            The local folder path.
        remote_folder : str
            The remote folder path in the bucket.
        partition_date : datetime.date
            The partition date.
        partition_name : str, optional
            The name of the partition column. Defaults to 'date'.
        override : bool, optional
            Whether to override the destination partition. Defaults to ``False``.
        """
        ...

    @staticmethod
    def sync_from_local(
        bucket_name: str,
        data_set: str,
        table: str,
        local_folder: str,
        prefix: str,
        partition_date: datetime.date,
        partition_name: str = ...,
        file_mask: str = ...,
        override: bool = ...,
    ) -> None:
        """
        Upload local files then load them into BigQuery.

        Parameters
        ----------
        bucket_name : str
            The name of the Cloud Storage bucket.
        data_set : str
            The BigQuery dataset ID.
        table : str
            The BigQuery table ID.
        local_folder : str
            The local folder containing the data.
        prefix : str
            The prefix to use for the remote folder.
        partition_date : datetime.date
            The partition date.
        partition_name : str, optional
            The name of the partition column. Defaults to 'date'.
        file_mask : str, optional
            The file mask of the files to upload. Defaults to '*.csv.gz'.
        override : bool, optional
            Whether to override existing files in the bucket. Defaults to ``False``.
        """
        ...

    def bigquery_to_dataframe(self, query_string: str) -> pd.DataFrame:
        """
        Run a query and return the results as a DataFrame.

        Parameters
        ----------
        query_string : str
            The query to execute.

        Returns
        -------
        pandas.DataFrame
            A DataFrame containing the query results.
        """
        ...

    def dataframe_to_bigquery(
        self,
        dataframe: pd.DataFrame,
        table_id: str,
        write_disposition: Literal[
            "WRITE_TRUNCATE", "WRITE_APPEND", "WRITE_EMPTY"
        ] = ...,
    ) -> None:
        """
        Load a DataFrame into a BigQuery table.

        Parameters
        ----------
        dataframe : pandas.DataFrame
            The DataFrame to load.
        table_id : str
            The destination table ID.
        write_disposition : str, optional
            Specifies the action that occurs if the destination table already
            exists. Defaults to ``WRITE_TRUNCATE``.
        """
        ...
